{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from numpy import *\n",
    "import pandas as pd\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass classifier decision tree using ID3 algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the entire dataset prior to learning using min-max normalization \n",
    "def normalize(matrix):\n",
    "#   transfer the data metrix to np array in float type.\n",
    "    a=np.array(matrix).astype(float)\n",
    "#     print(\"normalizing the entire dataset:\")\n",
    "#     print(a)\n",
    "#     print(\"Before normalizing\")\n",
    "#   apply the normalization along the 0 axis of a using the formula: (x - x_min)/(x_max - x_min)\n",
    "    p1= a - a.min(axis=0)\n",
    "    p2= a.max(axis=0)-a.min(axis=0)\n",
    "    equation=p1/p2\n",
    "    return equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading from the file using numpy genfromtxt with delimiter ','\n",
    "def load_csv(file):\n",
    "    X=np.genfromtxt(file, delimiter=\",\",dtype=str)\n",
    "    return (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to randomly shuffle the array using the numpy.random.shuffle()\n",
    "def random_numpy_array(ar):\n",
    "    np.random.shuffle(ar)\n",
    "    return ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the data and generate the training labels,training features, test labels and test training\n",
    "def generate_set(X):\n",
    "#     print(X.shape[0])\n",
    "#     store the label X[:,-1] to Y\n",
    "    Y = X[:,-1]\n",
    "#     reshape y to (Y's length, 1)\n",
    "#     store it to j\n",
    "    j = Y.reshape(len(Y),1)\n",
    "    \n",
    "#     print(\"J is\",j)\n",
    "#     create the new_X which exclude the label X[:,:-1]\n",
    "    new_X = X[:,:-1]\n",
    "\n",
    "#     normalize the data step\n",
    "#     using our implemented function normalize()\n",
    "    normalized_new_X = normalize(new_X)\n",
    "\n",
    "#     add the label back to the normiazlied X\n",
    "#     using np.concatenate along axis=1\n",
    "    X = np.concatenate((normalized_new_X,j),axis=1)\n",
    "\n",
    "#     store the size of rows of the normalized X with labels\n",
    "    rows= X.shape[0]\n",
    "\n",
    "#     use the 10% of the data to be the test set.\n",
    "#     store the number of testing data\n",
    "    testingSet = round(rows * 0.1)\n",
    "\n",
    "#     set the starting idex to be 0\n",
    "    start = 0\n",
    "\n",
    "#     set the ending index to be the number of testing data\n",
    "    end = testingSet\n",
    "\n",
    "#     create a list that store all features of the testing data\n",
    "    testingFeatures = []\n",
    "\n",
    "\n",
    "#     create a list that store all labels of the testing data\n",
    "    testingLabels = []\n",
    "\n",
    "#     create a list that store all features of the training data\n",
    "    trainingFeatures = []\n",
    "\n",
    "#     create a list that store all labels of the training data\n",
    "    trainingLabels = []\n",
    "\n",
    "#     10-fold cross-validation:\n",
    "    for i in range(10):\n",
    "#         store the test set for corss-validation using X[start:end,:]\n",
    "        testSet = X[start:end,:]\n",
    "\n",
    "#         get training data before the testing data X[:start, :]\n",
    "        trainBeforetest = X[:start,:]\n",
    "\n",
    "#         get training data after the testing data X[:start, :]\n",
    "        trainAftertest = X[end:,:]\n",
    "\n",
    "#         form the new training set using np.concatenate\n",
    "        trainSet = np.concatenate((trainBeforetest,trainAftertest),axis=0)\n",
    "\n",
    "#         get the testing set labels\n",
    "        testingSetLabels = testSet[:,-1]\n",
    "\n",
    "#         flattent the labels\n",
    "        testingSetLabels.flatten()\n",
    "\n",
    "#         get the training set labels\n",
    "        trainSetLabels = trainSet[:,-1]\n",
    "\n",
    "#         flattent the labels\n",
    "        trainSetLabels.flatten()\n",
    "\n",
    "#         create the test set exclude the labels\n",
    "        testSet = testSet[:,:-1].astype(np.float)\n",
    "\n",
    "#         same for the training set\n",
    "        trainSet = trainSet[:,:-1].astype(np.float)\n",
    "\n",
    "\n",
    "\n",
    "#         append test data of this fold to the list\n",
    "#         append test lables of this fold to the list\n",
    "\n",
    "        testingFeatures.append(testSet)\n",
    "        testingLabels.append(testingSetLabels)\n",
    "#         do the same for the training set\n",
    "\n",
    "        trainingFeatures.append(trainSet)\n",
    "        trainingLabels.append(trainSetLabels)\n",
    "#         update the index pointer\n",
    "\n",
    "        start=end\n",
    "        end=end+testingSet\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     return the fold list that contain data and label for both training and testing set.\n",
    "    return trainingFeatures,trainingLabels,testingFeatures,testingLabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a dictionary where the key is the class label and values are the features which belong to that class.\n",
    "def build_dict_of_attributes_with_class_values(X,y):\n",
    "#     init a dict for attributes\n",
    "    attributedict = {}\n",
    "\n",
    "#     init feature list.\n",
    "    featurelist = []\n",
    "\n",
    "#     for each feature in the dataset\n",
    "    for i in range(X.shape[1]):\n",
    "#         store the featur index\n",
    "        featureindex = i\n",
    "#     find all the value correspond to this feature\n",
    "        values = X[:,i]\n",
    "#     init an attribute list\n",
    "        attributelist = []\n",
    "#     init the counter to 0\n",
    "        counter = 0\n",
    "    # for each value in the \"all the value correspond to this feature\"\n",
    "        for eachvalue in values:\n",
    "            #             init a empty list that store the attribute value\n",
    "            attributeValuesList = []\n",
    "            #             append the this value to the list\n",
    "            attributeValuesList.append(eachvalue)\n",
    "            #             append the label of this value to the list\n",
    "            attributeValuesList.append(y[counter])\n",
    "            #             append this list to the attribute list.\n",
    "            attributelist.append(attributeValuesList)\n",
    "            #             increase the counter\n",
    "            counter+=1\n",
    "#         add this attribute list to the dict according to the feature index\n",
    "        attributedict[featureindex] = attributelist\n",
    "#         append the feature indx to the feature list.\n",
    "        featurelist.append(featureindex)\n",
    "#     return the dict and feature list.\n",
    "    return attributedict,featurelist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative Dichotomiser 3 entropy calculation\n",
    "def entropy(y):\n",
    "#     init a class frequence dict\n",
    "    classFreq = {}\n",
    "\n",
    "#     init the attribute entropy to 0\n",
    "    attributeEntropy = 0\n",
    "\n",
    "#     for each label in y:\n",
    "    for i in y:\n",
    "#         this is label is already in the dict, we increase its feq\n",
    "        if i in classFreq:\n",
    "            classFreq[i]+=1\n",
    "#          else, we set the freq to 1\n",
    "        else:\n",
    "            classFreq[i] = 1\n",
    "\n",
    "#     calculate the cumulate entropy using the formula.\n",
    "    for x in classFreq.values():\n",
    "        p1= (-x/float(len(y)))\n",
    "        p2= math.log(x/float(len(y)),2)\n",
    "        attributeEntropy +=  p1*p2 \n",
    "\n",
    "    return attributeEntropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class node and explanation is self explaination\n",
    "class Node(object):\n",
    "#     init the node with val,lchild,rchild,thea and leaf.\n",
    "    def __init__(self, val, lchild, rchild,thea,leaf):\n",
    "        self.root_value = val\n",
    "        self.root_left = lchild\n",
    "        self.root_right = rchild\n",
    "        self.theta = thea\n",
    "        self.leaf = leaf\n",
    "\n",
    "#     method to identify if the node is leaf\n",
    "    def is_leaf(self):\n",
    "        \n",
    "        return self.leaf\n",
    "\n",
    "#     method to return threshold value\n",
    "    def ret_thetha(self):\n",
    "        \n",
    "        return self.theta\n",
    "    \n",
    "#     method return root value\n",
    "    def ret_root_value(self):\n",
    "        \n",
    "        return self.root_value\n",
    "    \n",
    "#     method return left tree\n",
    "    def ret_llist(self):\n",
    "        \n",
    "        return self.root_left\n",
    "\n",
    "#     method return right tree\n",
    "    def ret_rlist(self):\n",
    "        \n",
    "        return self.root_right\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"(%r, %r, %r, %r)\" %(self.root_value,self.root_left,self.root_right,self.theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree object\n",
    "class DecisionTree(object):\n",
    "#     init a variable called fea_list\n",
    "    fea_list = []\n",
    "\n",
    "#     init the Dtree by setting the root node to None\n",
    "    def __init__(self):\n",
    "        self.root_node = None\n",
    "\n",
    "    #method to return the major class value using Counter() and .most_common()\n",
    "    def cal_major_class_values(self,class_values):\n",
    "    \n",
    "        return Counter(class_values).most_common(1)[0][0]\n",
    "\n",
    "    #method to calculate best threshold value for each feature\n",
    "    def cal_best_theta_value(self,ke,attri_list):\n",
    "#         init a list for data\n",
    "        dataList = []\n",
    "#         init a list for class labes\n",
    "        classLabels= []\n",
    "\n",
    "#         for each attribute in the attri_list\n",
    "        for i in attri_list:\n",
    "#             append the data\n",
    "            dataList.append(i[0])       \n",
    "#             append the feature value.\n",
    "            classLabels.append(i[1])\n",
    "\n",
    "#         calculate the entropy of those feaure values\n",
    "        entropyResult = entropy(classLabels)\n",
    "\n",
    "#         init the max info gain = 0\n",
    "        maxInfoGain = 0\n",
    "\n",
    "#         init theta=0\n",
    "        theta=0\n",
    "\n",
    "#         init a list that store the best index on the left\n",
    "        bestIndexLeft = []\n",
    "\n",
    "#         init a list that store the best index on the right\n",
    "        bestIndexRight = []\n",
    "\n",
    "#         init a list that store class labels after split\n",
    "        classLabelsAS = []\n",
    "\n",
    "#         sort the data\n",
    "        dataList.sort()\n",
    "\n",
    "#         for each index of data:\n",
    "        for i in range(len(dataList)-1):\n",
    "\n",
    "#             calculate the current theta using data[i]+data[i+1])/ 2\n",
    "            currentTheta = float(dataList[i]+dataList[i+1])/2\n",
    "#             init a list that store index that less than theta\n",
    "            indexLT = []\n",
    "#             init a list that store value that less than theta\n",
    "            valueLT = []\n",
    "#             init a list that store index that greater than theta\n",
    "            indexGT = []\n",
    "#             init a list that store value that greater than theta\n",
    "            valueGT =[]\n",
    "#             init the counter to 0\n",
    "            counter = 0\n",
    "#             for each index and value in attri_list\n",
    "            for c,j in enumerate(attri_list):\n",
    "#                 if value less or equal than the current theta:\n",
    "#                 update the \"less\" list of index and value\n",
    "                if (j[0]<=currentTheta):\n",
    "                    indexLT.append(c)\n",
    "                    valueLT.append(j[1])\n",
    "#                 else update the \"greater\" list of index and value\n",
    "                else:\n",
    "                    indexGT.append(c)\n",
    "                    valueGT.append(j[1]) \n",
    "#           calculate the entropy of the \"less\" list\n",
    "            entropyLess= entropy(valueLT)\n",
    "#             calculate the entropy of the \"greater\" list\n",
    "            entropyGreat= entropy(valueGT)\n",
    "#             calculate the info gain using the formular.\n",
    "            a= len(indexLT)/len(classLabels)\n",
    "            b= len(indexGT)/len(classLabels)\n",
    "            infoGain= a * entropyLess\n",
    "            infoGain= infoGain+ (b*entropyGreat)\n",
    "            infoGain = entropyResult -infoGain\n",
    "#             if current info gain > max info gan\n",
    "            if infoGain> maxInfoGain:\n",
    "#                 update the info gain, \n",
    "                maxInfoGain=infoGain\n",
    "#               the theta, the best index list of right, the best index list of left \n",
    "                theta= currentTheta\n",
    "                bestIndexRight=indexGT\n",
    "                bestIndexLeft=indexLT\n",
    "#               class_labels_list_after_split\n",
    "                classLabelsAS= valueGT+valueLT\n",
    "\n",
    "#         return the max info gain, theata,the best left list,the best right list and class label after split\n",
    "        return maxInfoGain, theta, bestIndexLeft,bestIndexRight, classLabelsAS\n",
    "\n",
    "    #method to select the best feature out of all the features.\n",
    "    def best_feature(self,dict_rep):\n",
    "#         set key value to none\n",
    "        keyValue=None\n",
    "\n",
    "#         set best info gain to -1\n",
    "        bestInfoGain=-1\n",
    "\n",
    "#         set best theta to 0\n",
    "        bestTheta=0\n",
    "\n",
    "#         set best left list to empty\n",
    "        bestLeftList=[]\n",
    "\n",
    "#         set best right list to empty\n",
    "        bestRightList=[]\n",
    "\n",
    "#         set best class labels after split to empty\n",
    "        bestClassLabelsAS=[]\n",
    "\n",
    "#         init a result list\n",
    "        resultList=[]\n",
    "\n",
    "#         for each key in dict_rep:\n",
    "        for ke in dict_rep.keys():\n",
    "#             using cal_best_theta_value() and store all returned values\n",
    "            maxInfoGain, theta, bestIndexLeft,bestIndexRight, classLabelsAS= self.cal_best_theta_value(ke, dict_rep[ke])\n",
    "            \n",
    "\n",
    "#             if info gian is greater than best info gain:\n",
    "            if maxInfoGain> bestInfoGain:\n",
    "\n",
    "#                 update info gain, theth, key value,\n",
    "#                     left list, right list, class labels after split\n",
    "                bestInfoGain=maxInfoGain\n",
    "                bestTheta=theta\n",
    "                keyValue= ke\n",
    "                bestLeftList=bestIndexLeft\n",
    "                bestRightList=bestIndexRight\n",
    "                bestClassLabelsAS= classLabelsAS\n",
    "\n",
    "\n",
    "\n",
    "    #         append the key value to the retrun list\n",
    "        resultList.append(keyValue)\n",
    "    #         append the theta value to the retrun list\n",
    "        resultList.append(bestTheta)\n",
    "    #         append the left list to the retrun list\n",
    "        resultList.append(bestLeftList)\n",
    "    #         append the right list to the retrun list\n",
    "        resultList.append(bestRightList)\n",
    "    #         append the class labels to the retrun list\n",
    "        resultList.append(bestClassLabelsAS)\n",
    "#         return the list.\n",
    "        return resultList\n",
    "\n",
    "    def get_remainder_dict(self,dict_of_everything,index_split):\n",
    "        global fea_list\n",
    "#         init a split dict\n",
    "        split_dict={}\n",
    "\n",
    "#         for each key \"ke\" in dict_of_everything:\n",
    "        for ke in dict_of_everything.keys():\n",
    "#             init a value list\n",
    "            valueList=[]\n",
    "#             init a modified list\n",
    "            modifiedList=[]\n",
    "#             get the corresponding values of the key\"ke\" \n",
    "            keValues= dict_of_everything[ke]\n",
    "#             for each value and its corresponding index of the key\"ke\" \n",
    "            for eachvalue in range(len(keValues)):\n",
    "\n",
    "#                 if index is not in the index_split:\n",
    "                if eachvalue not in index_split:\n",
    "#                     append it to the modified list and value list\n",
    "                    modifiedList.append(keValues[eachvalue])\n",
    "                    valueList.append(keValues[eachvalue][1])\n",
    "#             add this modified list to the dict\n",
    "            split_dict[ke]=modifiedList\n",
    "\n",
    "#         return the splited dict and val list\n",
    "        return split_dict, valueList\n",
    "\n",
    "    #method to create decision tree\n",
    "    def create_decision_tree(self, dict_of_everything,class_val,eta_min_val):\n",
    "        global fea_list\n",
    "        #if all the class labels are same, then we are set\n",
    "        if len(set(class_val)) ==1:\n",
    "#             print(\"Leaf node for set class is\",class_val[0],len(class_val))\n",
    "            n= Node(class_val[0],None,None,0,True)\n",
    "            return n\n",
    "        #if the no class vales are less than threshold, we assign the class with max values as the class label    \n",
    "        elif len(class_val) < eta_min_val:\n",
    "            classLabel = self.cal_major_class_values(class_val)\n",
    "            m = Node(classLabel,None,None,0,True)\n",
    "            return m\n",
    "#         else:\n",
    "        else:\n",
    "#             using the best_feature to get best feature list\n",
    "            bestFeatureList = self.best_feature(dict_of_everything)\n",
    "\n",
    "#             store the node name, theta,left split, right split and class labes\n",
    "            nodeName = bestFeatureList[0]\n",
    "            theta = bestFeatureList[1]\n",
    "            leftSplit = bestFeatureList[2]\n",
    "            rightSplit = bestFeatureList[3]\n",
    "            labels = bestFeatureList[4]\n",
    "\n",
    "#             call get_remainder_dict to get left tree data\n",
    "            leftSD, leftSC= self.get_remainder_dict(dict_of_everything, leftSplit)\n",
    "\n",
    "#             call get_remainder_dict to get right tree data\n",
    "            rightSD, rightSC= self.get_remainder_dict(dict_of_everything, rightSplit)\n",
    "\n",
    "#             call create_decision_tree to get left tree based on the left tree data\n",
    "            leftSN = self.create_decision_tree(leftSD,leftSC,eta_min_val)\n",
    "\n",
    "#             call create_decision_tree to get right tree based on therightleft tree data\n",
    "            rightSN = self.create_decision_tree(rightSD, rightSC, eta_min_val)\n",
    "\n",
    "#             set the root node\n",
    "            rn= Node(nodeName, rightSN, leftSN, theta, False)\n",
    "\n",
    "#             return root node\n",
    "            return rn\n",
    "            \n",
    "    #fit the decisin tree\n",
    "    def fit(self, dict_of_everything,cl_val,features,eta_min_val):\n",
    "#         set the fea_list the value of features\n",
    "        global fea_list\n",
    "        fea_list=features\n",
    "#         set the root node using the function create_decision_tree()\n",
    "        self.root_node= self.create_decision_tree(dict_of_everything,cl_val,eta_min_val)\n",
    "\n",
    "        return self.root_node\n",
    "\n",
    "    def classify(self,row,root):\n",
    "#         init the test dict\n",
    "        testDict={}\n",
    "#         add row to the dict\n",
    "        for k,j in enumerate(row): \n",
    "            testDict[k] = j\n",
    "#         set the current node to root\n",
    "        current_node = root\n",
    "#         while the current node is not leaf:\n",
    "        while not current_node.leaf:\n",
    "#             implement the case whether the current shoud go to the left\n",
    "            if testDict[current_node.root_value]<current_node.theta:\n",
    "                current_node  = current_node.root_left\n",
    "#             implement the case whether the current shoud go to the right\n",
    "            else:\n",
    "                current_node = current_node.root_right\n",
    "\n",
    "        \n",
    "#         return the calss of the current node\n",
    "        return current_node.root_value\n",
    "        \n",
    "    #method to the labels for the test data\n",
    "    def predict(self, X, root):\n",
    "#         predict using the classify()\n",
    "        PredictionList=[]\n",
    "        for i in X:\n",
    "            PredictionList.append(self.classify(i , root))\n",
    "        return PredictionList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the predicited accuracy\n",
    "def accuracy_for_predicted_values(test_class_names1,l):\n",
    "#     init true and false count to 0\n",
    "    falseCount=0\n",
    "    trueCount=0\n",
    "#     for each prediction,if predict is correct then, true++ else, false++\n",
    "    for i in range(len(test_class_names1)):\n",
    "        if test_class_names1[i] == l[i]:\n",
    "            trueCount+=1\n",
    "        else :\n",
    "            falseCount+=1\n",
    "#     print(trueCount)\n",
    "#     print(trueCount+ falseCount)\n",
    "#     print(len(test_class_names1))\n",
    "    Accuracy= trueCount/(len(test_class_names1))    \n",
    "#     return the acc\n",
    "    return Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_arr, eta_min):\n",
    "    eta_min_val = round(eta_min*num_arr.shape[0])\n",
    "    #randomly shuffle the array so that we can divide the data into test/training\n",
    "    randomArray = random_numpy_array(num_arr)\n",
    "    #divide data into test labels,test features,training labels, training features\n",
    "    trainingFeatures,trainingLabels, testingFeatures,testingLabels = generate_set(randomArray)\n",
    "    \n",
    "#     init cumulate acc to 0\n",
    "    CumulateAccuracy = 0\n",
    "    AccuracyList = []\n",
    "    #ten fold iteration \n",
    "    for i in range(10):\n",
    "        #build a dictionary with class labels and respective features values belonging to that class\n",
    "        attributeDict,featureList = build_dict_of_attributes_with_class_values(trainingFeatures[i],trainingLabels[i])\n",
    "        #instantiate decision tree instance\n",
    "        Decisiontree = DecisionTree()\n",
    "        # build the decision tree model.\n",
    "        Decisiontree.fit(attributeDict,trainingLabels[i],featureList,eta_min_val)\n",
    "        #predict the class labels for test features\n",
    "        predictedClassLabels = Decisiontree.predict(testingFeatures[i],Decisiontree.root_node)\n",
    "        #calculate the accuracy for the predicted values \n",
    "        accuracy = accuracy_for_predicted_values(predictedClassLabels,testingLabels[i])\n",
    "        AccuracyList.append(accuracy)  \n",
    "#         add acc to cumulate acc\n",
    "        CumulateAccuracy+=accuracy\n",
    "\n",
    "        print(\"Accuracy is \",accuracy*100, \" %\")\n",
    "    print(\"Accuracy across 10-cross validation for\",eta_min,\"is\",(float(CumulateAccuracy)/10)*100, \" %\")\n",
    "    print(\"Standard dev for\", eta_min, \"is\", np.std(AccuracyList))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  93.33333333333333  %\n",
      "Accuracy is  93.33333333333333  %\n",
      "Accuracy is  86.66666666666667  %\n",
      "Accuracy is  100.0  %\n",
      "Accuracy is  100.0  %\n",
      "Accuracy is  93.33333333333333  %\n",
      "Accuracy is  93.33333333333333  %\n",
      "Accuracy is  100.0  %\n",
      "Accuracy is  100.0  %\n",
      "Accuracy is  100.0  %\n",
      "Accuracy across 10-cross validation for 0.05 is 96.00000000000001  %\n",
      "Standard dev for 0.05 is 0.044221663871405324\n",
      "Accuracy is  100.0  %\n",
      "Accuracy is  93.33333333333333  %\n",
      "Accuracy is  93.33333333333333  %\n",
      "Accuracy is  93.33333333333333  %\n",
      "Accuracy is  86.66666666666667  %\n",
      "Accuracy is  93.33333333333333  %\n",
      "Accuracy is  100.0  %\n",
      "Accuracy is  100.0  %\n",
      "Accuracy is  100.0  %\n",
      "Accuracy is  93.33333333333333  %\n",
      "Accuracy across 10-cross validation for 0.1 is 95.33333333333334  %\n",
      "Standard dev for 0.1 is 0.04268749491621898\n",
      "Accuracy is  93.33333333333333  %\n",
      "Accuracy is  100.0  %\n",
      "Accuracy is  100.0  %\n",
      "Accuracy is  100.0  %\n",
      "Accuracy is  93.33333333333333  %\n",
      "Accuracy is  80.0  %\n",
      "Accuracy is  86.66666666666667  %\n",
      "Accuracy is  100.0  %\n",
      "Accuracy is  93.33333333333333  %\n",
      "Accuracy is  100.0  %\n",
      "Accuracy across 10-cross validation for 0.15 is 94.66666666666667  %\n",
      "Standard dev for 0.15 is 0.06531972647421808\n",
      "Accuracy is  93.33333333333333  %\n",
      "Accuracy is  100.0  %\n",
      "Accuracy is  93.33333333333333  %\n",
      "Accuracy is  100.0  %\n",
      "Accuracy is  86.66666666666667  %\n",
      "Accuracy is  93.33333333333333  %\n",
      "Accuracy is  86.66666666666667  %\n",
      "Accuracy is  100.0  %\n",
      "Accuracy is  93.33333333333333  %\n",
      "Accuracy is  93.33333333333333  %\n",
      "Accuracy across 10-cross validation for 0.2 is 94.0  %\n",
      "Standard dev for 0.2 is 0.046666666666666655\n",
      "Accuracy is  93.33333333333333  %\n",
      "Accuracy is  100.0  %\n",
      "Accuracy is  93.33333333333333  %\n",
      "Accuracy is  93.33333333333333  %\n",
      "Accuracy is  86.66666666666667  %\n",
      "Accuracy is  100.0  %\n",
      "Accuracy is  100.0  %\n",
      "Accuracy is  93.33333333333333  %\n",
      "Accuracy is  93.33333333333333  %\n",
      "Accuracy is  93.33333333333333  %\n",
      "Accuracy across 10-cross validation for 0.25 is 94.66666666666667  %\n",
      "Standard dev for 0.25 is 0.039999999999999994\n"
     ]
    }
   ],
   "source": [
    "eta_min_list = [0.05,0.10,0.15,0.20,0.25]\n",
    "newfile = \"iris.csv\"\n",
    "num_arr = load_csv(newfile)\n",
    "for i in eta_min_list:\n",
    "    main(num_arr,i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  85.21739130434783  %\n",
      "Accuracy is  81.52173913043478  %\n",
      "Accuracy is  66.08695652173913  %\n",
      "Accuracy is  47.391304347826086  %\n",
      "Accuracy is  85.86956521739131  %\n",
      "Accuracy is  43.26086956521739  %\n",
      "Accuracy is  53.2608695652174  %\n",
      "Accuracy is  74.1304347826087  %\n",
      "Accuracy is  56.52173913043478  %\n",
      "Accuracy is  83.69565217391305  %\n",
      "Accuracy across 10-cross validation for 0.05 is 67.69565217391305  %\n",
      "Standard dev for 0.05 is 0.15719420760190778\n",
      "Accuracy is  87.60869565217392  %\n",
      "Accuracy is  42.608695652173914  %\n",
      "Accuracy is  91.08695652173913  %\n",
      "Accuracy is  38.04347826086957  %\n",
      "Accuracy is  84.1304347826087  %\n",
      "Accuracy is  43.47826086956522  %\n",
      "Accuracy is  88.47826086956522  %\n",
      "Accuracy is  89.78260869565217  %\n",
      "Accuracy is  82.82608695652173  %\n",
      "Accuracy is  66.95652173913044  %\n",
      "Accuracy across 10-cross validation for 0.1 is 71.50000000000001  %\n",
      "Standard dev for 0.1 is 0.20768653218775143\n",
      "Accuracy is  86.08695652173914  %\n",
      "Accuracy is  41.95652173913044  %\n",
      "Accuracy is  84.56521739130434  %\n",
      "Accuracy is  83.91304347826087  %\n",
      "Accuracy is  47.391304347826086  %\n",
      "Accuracy is  64.13043478260869  %\n",
      "Accuracy is  87.39130434782608  %\n",
      "Accuracy is  84.78260869565217  %\n",
      "Accuracy is  87.82608695652175  %\n",
      "Accuracy is  85.86956521739131  %\n",
      "Accuracy across 10-cross validation for 0.15 is 75.3913043478261  %\n",
      "Standard dev for 0.15 is 0.16722690967506007\n",
      "Accuracy is  85.65217391304348  %\n",
      "Accuracy is  82.17391304347827  %\n",
      "Accuracy is  88.04347826086956  %\n",
      "Accuracy is  85.21739130434783  %\n",
      "Accuracy is  39.130434782608695  %\n",
      "Accuracy is  80.43478260869566  %\n",
      "Accuracy is  85.0  %\n",
      "Accuracy is  85.86956521739131  %\n",
      "Accuracy is  40.869565217391305  %\n",
      "Accuracy is  86.30434782608695  %\n",
      "Accuracy across 10-cross validation for 0.2 is 75.8695652173913  %\n",
      "Standard dev for 0.2 is 0.18052381250380065\n",
      "Accuracy is  42.608695652173914  %\n",
      "Accuracy is  83.26086956521739  %\n",
      "Accuracy is  84.34782608695653  %\n",
      "Accuracy is  81.52173913043478  %\n",
      "Accuracy is  84.56521739130434  %\n",
      "Accuracy is  85.0  %\n",
      "Accuracy is  81.30434782608695  %\n",
      "Accuracy is  83.69565217391305  %\n",
      "Accuracy is  83.69565217391305  %\n",
      "Accuracy is  83.69565217391305  %\n",
      "Accuracy across 10-cross validation for 0.25 is 79.36956521739131  %\n",
      "Standard dev for 0.25 is 0.12306517706964618\n",
      "CPU times: user 15h 27min 8s, sys: 1min 18s, total: 15h 28min 26s\n",
      "Wall time: 15h 30min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "eta_min_list = [0.05,0.10,0.15,0.20,0.25]\n",
    "newfile = \"spambase.csv\"\n",
    "num_arr = load_csv(newfile)\n",
    "for i in eta_min_list:\n",
    "    main(num_arr,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
